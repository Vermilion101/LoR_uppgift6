{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq_1</th>\n",
       "      <th>Freq_2</th>\n",
       "      <th>Freq_3</th>\n",
       "      <th>Freq_4</th>\n",
       "      <th>Freq_5</th>\n",
       "      <th>Freq_6</th>\n",
       "      <th>Freq_7</th>\n",
       "      <th>Freq_8</th>\n",
       "      <th>Freq_9</th>\n",
       "      <th>Freq_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Freq_52</th>\n",
       "      <th>Freq_53</th>\n",
       "      <th>Freq_54</th>\n",
       "      <th>Freq_55</th>\n",
       "      <th>Freq_56</th>\n",
       "      <th>Freq_57</th>\n",
       "      <th>Freq_58</th>\n",
       "      <th>Freq_59</th>\n",
       "      <th>Freq_60</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Freq_1  Freq_2  Freq_3  Freq_4  Freq_5  Freq_6  Freq_7  Freq_8  Freq_9  \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "     Freq_10  ...  Freq_52  Freq_53  Freq_54  Freq_55  Freq_56  Freq_57  \\\n",
       "0     0.2111  ...   0.0027   0.0065   0.0159   0.0072   0.0167   0.0180   \n",
       "1     0.2872  ...   0.0084   0.0089   0.0048   0.0094   0.0191   0.0140   \n",
       "2     0.6194  ...   0.0232   0.0166   0.0095   0.0180   0.0244   0.0316   \n",
       "3     0.1264  ...   0.0121   0.0036   0.0150   0.0085   0.0073   0.0050   \n",
       "4     0.4459  ...   0.0031   0.0054   0.0105   0.0110   0.0015   0.0072   \n",
       "..       ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "203   0.2684  ...   0.0116   0.0098   0.0199   0.0033   0.0101   0.0065   \n",
       "204   0.2154  ...   0.0061   0.0093   0.0135   0.0063   0.0063   0.0034   \n",
       "205   0.2529  ...   0.0160   0.0029   0.0051   0.0062   0.0089   0.0140   \n",
       "206   0.2354  ...   0.0086   0.0046   0.0126   0.0036   0.0035   0.0034   \n",
       "207   0.2354  ...   0.0146   0.0129   0.0047   0.0039   0.0061   0.0040   \n",
       "\n",
       "     Freq_58  Freq_59  Freq_60  Label  \n",
       "0     0.0084   0.0090   0.0032      R  \n",
       "1     0.0049   0.0052   0.0044      R  \n",
       "2     0.0164   0.0095   0.0078      R  \n",
       "3     0.0044   0.0040   0.0117      R  \n",
       "4     0.0048   0.0107   0.0094      R  \n",
       "..       ...      ...      ...    ...  \n",
       "203   0.0115   0.0193   0.0157      M  \n",
       "204   0.0032   0.0062   0.0067      M  \n",
       "205   0.0138   0.0077   0.0031      M  \n",
       "206   0.0079   0.0036   0.0048      M  \n",
       "207   0.0036   0.0061   0.0115      M  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sonar.all-data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Freq_1   208 non-null    float64\n",
      " 1   Freq_2   208 non-null    float64\n",
      " 2   Freq_3   208 non-null    float64\n",
      " 3   Freq_4   208 non-null    float64\n",
      " 4   Freq_5   208 non-null    float64\n",
      " 5   Freq_6   208 non-null    float64\n",
      " 6   Freq_7   208 non-null    float64\n",
      " 7   Freq_8   208 non-null    float64\n",
      " 8   Freq_9   208 non-null    float64\n",
      " 9   Freq_10  208 non-null    float64\n",
      " 10  Freq_11  208 non-null    float64\n",
      " 11  Freq_12  208 non-null    float64\n",
      " 12  Freq_13  208 non-null    float64\n",
      " 13  Freq_14  208 non-null    float64\n",
      " 14  Freq_15  208 non-null    float64\n",
      " 15  Freq_16  208 non-null    float64\n",
      " 16  Freq_17  208 non-null    float64\n",
      " 17  Freq_18  208 non-null    float64\n",
      " 18  Freq_19  208 non-null    float64\n",
      " 19  Freq_20  208 non-null    float64\n",
      " 20  Freq_21  208 non-null    float64\n",
      " 21  Freq_22  208 non-null    float64\n",
      " 22  Freq_23  208 non-null    float64\n",
      " 23  Freq_24  208 non-null    float64\n",
      " 24  Freq_25  208 non-null    float64\n",
      " 25  Freq_26  208 non-null    float64\n",
      " 26  Freq_27  208 non-null    float64\n",
      " 27  Freq_28  208 non-null    float64\n",
      " 28  Freq_29  208 non-null    float64\n",
      " 29  Freq_30  208 non-null    float64\n",
      " 30  Freq_31  208 non-null    float64\n",
      " 31  Freq_32  208 non-null    float64\n",
      " 32  Freq_33  208 non-null    float64\n",
      " 33  Freq_34  208 non-null    float64\n",
      " 34  Freq_35  208 non-null    float64\n",
      " 35  Freq_36  208 non-null    float64\n",
      " 36  Freq_37  208 non-null    float64\n",
      " 37  Freq_38  208 non-null    float64\n",
      " 38  Freq_39  208 non-null    float64\n",
      " 39  Freq_40  208 non-null    float64\n",
      " 40  Freq_41  208 non-null    float64\n",
      " 41  Freq_42  208 non-null    float64\n",
      " 42  Freq_43  208 non-null    float64\n",
      " 43  Freq_44  208 non-null    float64\n",
      " 44  Freq_45  208 non-null    float64\n",
      " 45  Freq_46  208 non-null    float64\n",
      " 46  Freq_47  208 non-null    float64\n",
      " 47  Freq_48  208 non-null    float64\n",
      " 48  Freq_49  208 non-null    float64\n",
      " 49  Freq_50  208 non-null    float64\n",
      " 50  Freq_51  208 non-null    float64\n",
      " 51  Freq_52  208 non-null    float64\n",
      " 52  Freq_53  208 non-null    float64\n",
      " 53  Freq_54  208 non-null    float64\n",
      " 54  Freq_55  208 non-null    float64\n",
      " 55  Freq_56  208 non-null    float64\n",
      " 56  Freq_57  208 non-null    float64\n",
      " 57  Freq_58  208 non-null    float64\n",
      " 58  Freq_59  208 non-null    float64\n",
      " 59  Freq_60  208 non-null    float64\n",
      " 60  Label    208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 99.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq_1</th>\n",
       "      <th>Freq_2</th>\n",
       "      <th>Freq_3</th>\n",
       "      <th>Freq_4</th>\n",
       "      <th>Freq_5</th>\n",
       "      <th>Freq_6</th>\n",
       "      <th>Freq_7</th>\n",
       "      <th>Freq_8</th>\n",
       "      <th>Freq_9</th>\n",
       "      <th>Freq_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Freq_51</th>\n",
       "      <th>Freq_52</th>\n",
       "      <th>Freq_53</th>\n",
       "      <th>Freq_54</th>\n",
       "      <th>Freq_55</th>\n",
       "      <th>Freq_56</th>\n",
       "      <th>Freq_57</th>\n",
       "      <th>Freq_58</th>\n",
       "      <th>Freq_59</th>\n",
       "      <th>Freq_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Freq_1      Freq_2      Freq_3      Freq_4      Freq_5      Freq_6  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \n",
       "std      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \n",
       "min      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n",
       "25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n",
       "50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n",
       "75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \n",
       "max      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n",
       "\n",
       "           Freq_7      Freq_8      Freq_9     Freq_10  ...     Freq_51  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \n",
       "mean     0.121747    0.134799    0.178003    0.208259  ...    0.016069   \n",
       "std      0.061788    0.085152    0.118387    0.134416  ...    0.012008   \n",
       "min      0.003300    0.005500    0.007500    0.011300  ...    0.000000   \n",
       "25%      0.080900    0.080425    0.097025    0.111275  ...    0.008425   \n",
       "50%      0.106950    0.112100    0.152250    0.182400  ...    0.013900   \n",
       "75%      0.154000    0.169600    0.233425    0.268700  ...    0.020825   \n",
       "max      0.372900    0.459000    0.682800    0.710600  ...    0.100400   \n",
       "\n",
       "          Freq_52     Freq_53     Freq_54     Freq_55     Freq_56     Freq_57  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.013420    0.010709    0.010941    0.009290    0.008222    0.007820   \n",
       "std      0.009634    0.007060    0.007301    0.007088    0.005736    0.005785   \n",
       "min      0.000800    0.000500    0.001000    0.000600    0.000400    0.000300   \n",
       "25%      0.007275    0.005075    0.005375    0.004150    0.004400    0.003700   \n",
       "50%      0.011400    0.009550    0.009300    0.007500    0.006850    0.005950   \n",
       "75%      0.016725    0.014900    0.014500    0.012100    0.010575    0.010425   \n",
       "max      0.070900    0.039000    0.035200    0.044700    0.039400    0.035500   \n",
       "\n",
       "          Freq_58     Freq_59     Freq_60  \n",
       "count  208.000000  208.000000  208.000000  \n",
       "mean     0.007949    0.007941    0.006507  \n",
       "std      0.006470    0.006181    0.005031  \n",
       "min      0.000300    0.000100    0.000600  \n",
       "25%      0.003600    0.003675    0.003100  \n",
       "50%      0.005800    0.006400    0.005300  \n",
       "75%      0.010350    0.010325    0.008525  \n",
       "max      0.044000    0.036400    0.043900  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Label', ylabel='count'>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=df[\"Label\"])\n",
    "# not a perfect balance of labels, but close enough to not make a massive difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features (X), and label (y)\n",
    "X = df.drop(\"Label\",axis=1)\n",
    "y = df[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PipeLine-KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [(\"scaler\",scaler),(\"knn\",knn)] # operations is list of tuples where\n",
    "pipe_std_knn_class = Pipeline(operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN hyperparameters\n",
    "k_values = list(np.arange(1,20))\n",
    "algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "weights = [\"uniform\", \"distance\"]\n",
    "metric = ['euclidean', 'l1', 'minkowski', 'chebyshev', 'hamming', 'cosine']\n",
    "\n",
    "# GridSearchCV param_grid\n",
    "param_grid = {\"knn__n_neighbors\":k_values,\n",
    "              \"knn__algorithm\":algorithm,\n",
    "              \"knn__weights\":weights,\n",
    "              \"knn__metric\":metric}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best_model = GridSearchCV(estimator=pipe_std_knn_class,\n",
    "                              param_grid=param_grid,\n",
    "                              cv=10,\n",
    "                              scoring=\"accuracy\",\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "1140 fits failed out of a total of 9120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "380 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 238, in fit\n",
      "    return self._fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 520, in _fit\n",
      "    self._check_algorithm_metric()\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 454, in _check_algorithm_metric\n",
      "    raise ValueError(\n",
      "ValueError: Metric 'cosine' not valid. Use sorted(sklearn.neighbors.VALID_METRICS['ball_tree']) to get valid options. Metric can also be a callable function.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "380 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 238, in fit\n",
      "    return self._fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 520, in _fit\n",
      "    self._check_algorithm_metric()\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 454, in _check_algorithm_metric\n",
      "    raise ValueError(\n",
      "ValueError: Metric 'hamming' not valid. Use sorted(sklearn.neighbors.VALID_METRICS['kd_tree']) to get valid options. Metric can also be a callable function.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "380 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py\", line 475, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 238, in fit\n",
      "    return self._fit(X, y)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 520, in _fit\n",
      "    self._check_algorithm_metric()\n",
      "  File \"c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 454, in _check_algorithm_metric\n",
      "    raise ValueError(\n",
      "ValueError: Metric 'cosine' not valid. Use sorted(sklearn.neighbors.VALID_METRICS['kd_tree']) to get valid options. Metric can also be a callable function.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Jesper H\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.84952381 0.84952381 0.80095238 0.84952381 0.84285714 0.85\n",
      " 0.77380952 0.83619048 0.7952381  0.80238095 0.75904762 0.82238095\n",
      " 0.76       0.78809524 0.72428571 0.76666667 0.71095238 0.74571429\n",
      " 0.67761905 0.75238095 0.7052381  0.7252381  0.68428571 0.73190476\n",
      " 0.69809524 0.7047619  0.69047619 0.71761905 0.69714286 0.70428571\n",
      " 0.67714286 0.71095238 0.70428571 0.69761905 0.69714286 0.71761905\n",
      " 0.71047619 0.73095238 0.87571429 0.87571429 0.80142857 0.87571429\n",
      " 0.82142857 0.83571429 0.77333333 0.83428571 0.79380952 0.80095238\n",
      " 0.7452381  0.78714286 0.7447619  0.79380952 0.73857143 0.77333333\n",
      " 0.75190476 0.75190476 0.73142857 0.76571429 0.72428571 0.73761905\n",
      " 0.71761905 0.73857143 0.71047619 0.72428571 0.73142857 0.7447619\n",
      " 0.73761905 0.75142857 0.72428571 0.73809524 0.74428571 0.75142857\n",
      " 0.71047619 0.7652381  0.74380952 0.75761905 0.84952381 0.84952381\n",
      " 0.80095238 0.84952381 0.84285714 0.85       0.77380952 0.83619048\n",
      " 0.7952381  0.80238095 0.75904762 0.82238095 0.76       0.78809524\n",
      " 0.72428571 0.76666667 0.71095238 0.74571429 0.67761905 0.75238095\n",
      " 0.7052381  0.7252381  0.68428571 0.73190476 0.69809524 0.7047619\n",
      " 0.69047619 0.71761905 0.69714286 0.70428571 0.67714286 0.71095238\n",
      " 0.70428571 0.69761905 0.69714286 0.71761905 0.71047619 0.73095238\n",
      " 0.76761905 0.76761905 0.76714286 0.76761905 0.74571429 0.75285714\n",
      " 0.7052381  0.76       0.75380952 0.76095238 0.69761905 0.77428571\n",
      " 0.73190476 0.73904762 0.68428571 0.74619048 0.7047619  0.71142857\n",
      " 0.67761905 0.73952381 0.69809524 0.71190476 0.64904762 0.72571429\n",
      " 0.68428571 0.71857143 0.66333333 0.73238095 0.67       0.69761905\n",
      " 0.62142857 0.67809524 0.64285714 0.67047619 0.58714286 0.67761905\n",
      " 0.6152381  0.64285714 0.48857143 0.48857143 0.57238095 0.55095238\n",
      " 0.51190476 0.51190476 0.51904762 0.51904762 0.47714286 0.47714286\n",
      " 0.55190476 0.53857143 0.53714286 0.53714286 0.51714286 0.53857143\n",
      " 0.51047619 0.51047619 0.53761905 0.53809524 0.58619048 0.58619048\n",
      " 0.55142857 0.57190476 0.55857143 0.55857143 0.54428571 0.53095238\n",
      " 0.53047619 0.53047619 0.53761905 0.53809524 0.53714286 0.53714286\n",
      " 0.5652381  0.53809524 0.55857143 0.55857143 0.82095238 0.82095238\n",
      " 0.80809524 0.82095238 0.8352381  0.8352381  0.8147619  0.82857143\n",
      " 0.79333333 0.82809524 0.78666667 0.8147619  0.77952381 0.80761905\n",
      " 0.75904762 0.8147619  0.73857143 0.80761905 0.71904762 0.79428571\n",
      " 0.73904762 0.78       0.71095238 0.76666667 0.70380952 0.75238095\n",
      " 0.68380952 0.76619048 0.70428571 0.75952381 0.7247619  0.77428571\n",
      " 0.7047619  0.76666667 0.7247619  0.75333333 0.7252381  0.76047619\n",
      " 0.84952381 0.84952381 0.80095238 0.84952381 0.84285714 0.85\n",
      " 0.77380952 0.83619048 0.7952381  0.80238095 0.75904762 0.82238095\n",
      " 0.76       0.78809524 0.72428571 0.76666667 0.71095238 0.74571429\n",
      " 0.67761905 0.75238095 0.7052381  0.7252381  0.68428571 0.73190476\n",
      " 0.69809524 0.7047619  0.69047619 0.71761905 0.69714286 0.70428571\n",
      " 0.67714286 0.71095238 0.70428571 0.69761905 0.69714286 0.71761905\n",
      " 0.71047619 0.73095238 0.87571429 0.87571429 0.80142857 0.87571429\n",
      " 0.82142857 0.83571429 0.77333333 0.83428571 0.79380952 0.80095238\n",
      " 0.7452381  0.78714286 0.7447619  0.79380952 0.73857143 0.77333333\n",
      " 0.75190476 0.75190476 0.73142857 0.76571429 0.72428571 0.73761905\n",
      " 0.71761905 0.73857143 0.71047619 0.72428571 0.73142857 0.7447619\n",
      " 0.73761905 0.75142857 0.72428571 0.73809524 0.74428571 0.75142857\n",
      " 0.71047619 0.7652381  0.74380952 0.75761905 0.84952381 0.84952381\n",
      " 0.80095238 0.84952381 0.84285714 0.85       0.77380952 0.83619048\n",
      " 0.7952381  0.80238095 0.75904762 0.82238095 0.76       0.78809524\n",
      " 0.72428571 0.76666667 0.71095238 0.74571429 0.67761905 0.75238095\n",
      " 0.7052381  0.7252381  0.68428571 0.73190476 0.69809524 0.7047619\n",
      " 0.69047619 0.71761905 0.69714286 0.70428571 0.67714286 0.71095238\n",
      " 0.70428571 0.69761905 0.69714286 0.71761905 0.71047619 0.73095238\n",
      " 0.76761905 0.76761905 0.76714286 0.76761905 0.73904762 0.74619048\n",
      " 0.71190476 0.76       0.75380952 0.76095238 0.69761905 0.77428571\n",
      " 0.73190476 0.73904762 0.68428571 0.74619048 0.7047619  0.71142857\n",
      " 0.67761905 0.73952381 0.69809524 0.71190476 0.64904762 0.72571429\n",
      " 0.68428571 0.71857143 0.66333333 0.73238095 0.67       0.69761905\n",
      " 0.62142857 0.67809524 0.64285714 0.67047619 0.58714286 0.67761905\n",
      " 0.6152381  0.64285714 0.48285714 0.48285714 0.48428571 0.48333333\n",
      " 0.5052381  0.5052381  0.47857143 0.49809524 0.45571429 0.45571429\n",
      " 0.49809524 0.5047619  0.48333333 0.48333333 0.51047619 0.51142857\n",
      " 0.48285714 0.48285714 0.51095238 0.49047619 0.48857143 0.48857143\n",
      " 0.50285714 0.48238095 0.53761905 0.53761905 0.54380952 0.54333333\n",
      " 0.48952381 0.48952381 0.46761905 0.45380952 0.51666667 0.51666667\n",
      " 0.52238095 0.50904762 0.50095238 0.50095238        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84952381 0.84952381 0.80095238 0.84952381 0.84285714 0.85\n",
      " 0.77380952 0.83619048 0.7952381  0.80238095 0.75904762 0.82238095\n",
      " 0.76       0.78809524 0.72428571 0.76666667 0.71095238 0.74571429\n",
      " 0.67761905 0.75238095 0.7052381  0.7252381  0.68428571 0.73190476\n",
      " 0.69809524 0.7047619  0.69047619 0.71761905 0.69714286 0.70428571\n",
      " 0.67714286 0.71095238 0.70428571 0.69761905 0.69714286 0.71761905\n",
      " 0.71047619 0.73095238 0.87571429 0.87571429 0.80142857 0.87571429\n",
      " 0.82142857 0.83571429 0.77333333 0.83428571 0.79380952 0.80095238\n",
      " 0.7452381  0.78714286 0.7447619  0.79380952 0.73857143 0.77333333\n",
      " 0.75190476 0.75190476 0.73142857 0.76571429 0.72428571 0.73761905\n",
      " 0.71761905 0.73857143 0.71047619 0.72428571 0.73142857 0.7447619\n",
      " 0.73761905 0.75142857 0.72428571 0.73809524 0.74428571 0.75142857\n",
      " 0.71047619 0.7652381  0.74380952 0.75761905 0.84952381 0.84952381\n",
      " 0.80095238 0.84952381 0.84285714 0.85       0.77380952 0.83619048\n",
      " 0.7952381  0.80238095 0.75904762 0.82238095 0.76       0.78809524\n",
      " 0.72428571 0.76666667 0.71095238 0.74571429 0.67761905 0.75238095\n",
      " 0.7052381  0.7252381  0.68428571 0.73190476 0.69809524 0.7047619\n",
      " 0.69047619 0.71761905 0.69714286 0.70428571 0.67714286 0.71095238\n",
      " 0.70428571 0.69761905 0.69714286 0.71761905 0.71047619 0.73095238\n",
      " 0.76761905 0.76761905 0.76714286 0.76761905 0.74571429 0.75285714\n",
      " 0.7052381  0.76       0.75380952 0.76095238 0.69761905 0.77428571\n",
      " 0.73190476 0.73904762 0.68428571 0.74619048 0.7047619  0.71142857\n",
      " 0.67761905 0.73952381 0.69142857 0.7052381  0.64904762 0.72571429\n",
      " 0.67761905 0.71190476 0.66333333 0.73238095 0.67       0.69761905\n",
      " 0.62142857 0.67809524 0.64952381 0.67714286 0.58714286 0.67761905\n",
      " 0.6152381  0.64285714        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.84952381 0.84952381 0.80095238 0.84952381 0.84285714 0.85\n",
      " 0.77380952 0.83619048 0.7952381  0.80238095 0.75904762 0.82238095\n",
      " 0.76       0.78809524 0.72428571 0.76666667 0.71095238 0.74571429\n",
      " 0.67761905 0.75238095 0.7052381  0.7252381  0.68428571 0.73190476\n",
      " 0.69809524 0.7047619  0.69047619 0.71761905 0.69714286 0.70428571\n",
      " 0.67714286 0.71095238 0.70428571 0.69761905 0.69714286 0.71761905\n",
      " 0.71047619 0.73095238 0.87571429 0.87571429 0.80142857 0.87571429\n",
      " 0.82142857 0.83571429 0.77333333 0.83428571 0.79380952 0.80095238\n",
      " 0.7452381  0.78714286 0.7447619  0.79380952 0.73857143 0.77333333\n",
      " 0.75190476 0.75190476 0.73142857 0.76571429 0.72428571 0.73761905\n",
      " 0.71761905 0.73857143 0.71047619 0.72428571 0.73142857 0.7447619\n",
      " 0.73761905 0.75142857 0.72428571 0.73809524 0.74428571 0.75142857\n",
      " 0.71047619 0.7652381  0.74380952 0.75761905 0.84952381 0.84952381\n",
      " 0.80095238 0.84952381 0.84285714 0.85       0.77380952 0.83619048\n",
      " 0.7952381  0.80238095 0.75904762 0.82238095 0.76       0.78809524\n",
      " 0.72428571 0.76666667 0.71095238 0.74571429 0.67761905 0.75238095\n",
      " 0.7052381  0.7252381  0.68428571 0.73190476 0.69809524 0.7047619\n",
      " 0.69047619 0.71761905 0.69714286 0.70428571 0.67714286 0.71095238\n",
      " 0.70428571 0.69761905 0.69714286 0.71761905 0.71047619 0.73095238\n",
      " 0.76761905 0.76761905 0.76714286 0.76761905 0.74571429 0.75285714\n",
      " 0.7052381  0.76       0.75380952 0.76095238 0.69761905 0.77428571\n",
      " 0.73190476 0.73904762 0.68428571 0.74619048 0.7047619  0.71142857\n",
      " 0.67761905 0.73952381 0.69809524 0.71190476 0.64904762 0.72571429\n",
      " 0.68428571 0.71857143 0.66333333 0.73238095 0.67       0.69761905\n",
      " 0.62142857 0.67809524 0.64285714 0.67047619 0.58714286 0.67761905\n",
      " 0.6152381  0.64285714 0.48857143 0.48857143 0.57238095 0.55095238\n",
      " 0.51190476 0.51190476 0.51904762 0.51904762 0.47714286 0.47714286\n",
      " 0.55190476 0.53857143 0.53714286 0.53714286 0.51714286 0.53857143\n",
      " 0.51047619 0.51047619 0.53761905 0.53809524 0.58619048 0.58619048\n",
      " 0.55142857 0.57190476 0.55857143 0.55857143 0.54428571 0.53095238\n",
      " 0.53047619 0.53047619 0.53761905 0.53809524 0.53714286 0.53714286\n",
      " 0.5652381  0.53809524 0.55857143 0.55857143 0.82095238 0.82095238\n",
      " 0.80809524 0.82095238 0.8352381  0.8352381  0.8147619  0.82857143\n",
      " 0.79333333 0.82809524 0.78666667 0.8147619  0.77952381 0.80761905\n",
      " 0.75904762 0.8147619  0.73857143 0.80761905 0.71904762 0.79428571\n",
      " 0.73904762 0.78       0.71095238 0.76666667 0.70380952 0.75238095\n",
      " 0.68380952 0.76619048 0.70428571 0.75952381 0.7247619  0.77428571\n",
      " 0.7047619  0.76666667 0.7247619  0.75333333 0.7252381  0.76047619]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-17 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-17 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-17 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-17 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-17 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-17 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-17 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-17 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-17 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-17 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-17 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-17 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-17 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-17 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-17 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;knn&#x27;, KNeighborsClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;knn__algorithm&#x27;: [&#x27;auto&#x27;, &#x27;ball_tree&#x27;, &#x27;kd_tree&#x27;,\n",
       "                                            &#x27;brute&#x27;],\n",
       "                         &#x27;knn__metric&#x27;: [&#x27;euclidean&#x27;, &#x27;l1&#x27;, &#x27;minkowski&#x27;,\n",
       "                                         &#x27;chebyshev&#x27;, &#x27;hamming&#x27;, &#x27;cosine&#x27;],\n",
       "                         &#x27;knn__n_neighbors&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n",
       "                                              12, 13, 14, 15, 16, 17, 18, 19],\n",
       "                         &#x27;knn__weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-65\" type=\"checkbox\" ><label for=\"sk-estimator-id-65\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                                       (&#x27;knn&#x27;, KNeighborsClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;knn__algorithm&#x27;: [&#x27;auto&#x27;, &#x27;ball_tree&#x27;, &#x27;kd_tree&#x27;,\n",
       "                                            &#x27;brute&#x27;],\n",
       "                         &#x27;knn__metric&#x27;: [&#x27;euclidean&#x27;, &#x27;l1&#x27;, &#x27;minkowski&#x27;,\n",
       "                                         &#x27;chebyshev&#x27;, &#x27;hamming&#x27;, &#x27;cosine&#x27;],\n",
       "                         &#x27;knn__n_neighbors&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n",
       "                                              12, 13, 14, 15, 16, 17, 18, 19],\n",
       "                         &#x27;knn__weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-66\" type=\"checkbox\" ><label for=\"sk-estimator-id-66\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;knn&#x27;, KNeighborsClassifier())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-67\" type=\"checkbox\" ><label for=\"sk-estimator-id-67\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-68\" type=\"checkbox\" ><label for=\"sk-estimator-id-68\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;KNeighborsClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier()</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('knn', KNeighborsClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'knn__algorithm': ['auto', 'ball_tree', 'kd_tree',\n",
       "                                            'brute'],\n",
       "                         'knn__metric': ['euclidean', 'l1', 'minkowski',\n",
       "                                         'chebyshev', 'hamming', 'cosine'],\n",
       "                         'knn__n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,\n",
       "                                              12, 13, 14, 15, 16, 17, 18, 19],\n",
       "                         'knn__weights': ['uniform', 'distance']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_best_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn__algorithm': 'auto',\n",
       " 'knn__metric': 'l1',\n",
       " 'knn__n_neighbors': 1,\n",
       " 'knn__weights': 'uniform'}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "knn_best_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           M       0.85      0.93      0.89        30\n",
      "           R       0.93      0.85      0.89        33\n",
      "\n",
      "    accuracy                           0.89        63\n",
      "   macro avg       0.89      0.89      0.89        63\n",
      "weighted avg       0.89      0.89      0.89        63\n",
      "\n",
      "Confusion Matrix\n",
      "[[28  2]\n",
      " [ 5 28]]\n",
      "\n",
      "Accuracy Score\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "y_pred_cv = knn_best_model.predict(X_test)\n",
    "print(f\"Classification Report\\n{classification_report(y_test,y_pred_cv)}\")\n",
    "print(f\"Confusion Matrix\\n{confusion_matrix(y_test,y_pred_cv)}\\n\")\n",
    "print(f\"Accuracy Score\\n{accuracy_score(y_test,y_pred_cv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affect of test size on accuracy score\n",
    "\n",
    "# Test size 0.35, acc_score 0.8767\n",
    "# Test size 0.30, acc_score 0.8888 ! (next best score, and bigger test size pool) (metric=l1)\n",
    "# Test size 0.25, acc_score 0.8846\n",
    "# Test size 0.20, acc_score 0.8333\n",
    "# Test size 0.15, acc_score 0.9375 !! (best score, but very low test size in numbers) (metrix=cosine)\n",
    "# Test size 0.10, acc_score 0.8095"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
